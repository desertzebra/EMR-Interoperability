
\section*{Experimental Setup}
\label{experimentalSetup}
In our earlier work \cite{Satti2020} $s_1$, $s_2$, and $s_5$ were used to generate over 115 million patient records, which are converted into a semi-structured form and stored in Hadoop Distributed File System (HDFS). We extended the same setup to create an additional 100,000 records, for 1000 patients with 3 medical fragments for $s_1$, $s_2$, and $s_4$, and 97 randomly selected and generated medical fragments amongst $s_1$, $s_2$, $s_3$, $s_4$ and $s_5$. These fragments, follow various design elements, supporting a variety of valid relational storage architectures. Such as, $s_1$, $s_2$ and $s_4$ are represented by creating a separate medical fragment for each participating table, $s_3$ utilizes its medical fragment to generate a linked record (from a linked object graph), where by the attributes can refer to other objects besides the elements of $t$, mimicking the application of explicit foreign keys, and $s_5$ is a flat table structure. The code to generate this data set is available at ``uhp\_map\_generation"\footnote{https://github.com/desertzebra/UHP\_v4/tree/main/uhpr\_storage}. This application produces three custom formatted files, containing an index for patients, an index for their medical fragments, and the medical fragment, corresponding to the EMR data. Using the medical fragments file, we then generate the AA\footnote{https://github.com/desertzebra/UHP\_v4/tree/main/uhp\_map\_generation}. The resulting set of AA are temporarily stored in a ``json" file, which is then read by the same application to partially generate the schema maps. This process, is used to create 21,809  distinct pairs of attributes, across $s$. Each pair also contains the ``relationshipList", which stores either the positive result of a case insensitive match between the attributes, in which case no further processing is performed, or the results of fuzzy string matching\cite{FuzzyWuzzy} \footnote{Java Library: https://github.com/xdrop/fuzzywuzzy} between the attribute names. 
The ``json" file thus produced, is then used by a python script to perform the syntactic and semantic matching on the attributes. In this script we used the pre-trained Bert base model with nli mean tokens\cite{reimers-2019-sentence-bert} to create the sentence embeddings, which are then compared using cosine similarity. 

The rationale behind switching the applications at various stages, is to cache the results and create checkpoints for restarting any failed stages, easily. Additionally, since python provides better support for easy generation of embedding vectors, it was thus preferred over the Java based implementation, which is otherwise very beneficial for other tools. These applications were executed on a workstation running Ubuntu 20.04.2 LTS on top of AMD Ryzen 3 2200G, and 32GB ram.

In order to compare our computed models with ground truth, \textbf{4} data annotators were hired, to anonymously, score the similarity of each pair of attribute names. In order to support this process, we first repurposed one of our generated data matrix, by marking all attribute pairs belonging to the same schema with the symbol ``-". Following this, the annotators, marked each cell corresponding to a pair of attributes, by determining the similarity in terms of dissimilar as ``0", exactly similar as ``1", row attribute as child of column attribute as ``<", row attribute as a parent of the column attribute as ``>", and finally, unknown as ``~". The data sheets generated after this extensive human effort, have been made available for other researchers\footnote{https://github.com/desertzebra/EMR-Interoperability/tree/master/Implemenation/Data/Annotated}. These sheets, additionally contain some missing values, which were left out by the annotators, but in order to maintain their originality, these values were not filled; instead during our evaluation for these datasets, the missing values were considered as having the score ``0". Using \textbf{Cohen's Kappa score (d)}, we evaluated the inter-rater agreement of these annotations, which have been visualized in \textbf{figKappaScore}. The 7 permutations, amongst the 4 annotators, generate 261 kappa scores, where each individual score corresponds to the correlation of agreement between two annotators. Further, each score identifies the agreement in terms of scoring a set of paired attributes, while keeping the row attribute constant, and moving along the 261 column attributes.

An average dataset was then produced, containing average scores of all annotators, for each attribute pair. This dataset was then used for evaluating the performance of the automated approaches.

