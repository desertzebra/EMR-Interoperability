
\section*{Methodology}
\label{methodology}
Healthcare interoperability, with a focus on non-standard compliant medical schema, is dependent on the generation and validation of schema maps, as discussed above. To this end, the creation of a cohesive workflow is of utmost importance. In our earlier work \cite{Satti2020} we used maximum sequence identification and matching using suffix trees for syntactic matching of two distinctly sources data schemas. This was followed by semantic concept enrichment and subsequently concept matching, for creating rules in the form of schema maps. The simplified mapping functions, thereby created, provided a simple methodology for converting semi-structured medical data, into a non-persisted, interpretable, model form.
In our current methodology we have utilized state-of-the-art machine learning techniques for converting medical schemas into semi-structured form, which is then used to create embedding vectors. These vectors are then used for modeling and creating interpretable rules for applying semantic reconciliation in the form of schema matching and/or transformation. These rules are managed by the RDR which allows fast inference using the relevant knowledge sub-tree, and can well manage knowledge evolution to scale out (by incorporating new schema) and up (by incorporating changes and additions to existing schema)
In essence, the aim here is to define a uniqueness property ($\mathbb{P}$), as shown in Equation \eqref{uniquenessProp}, whereby the semantic and syntactic uniqueness of each attribute as a disjoint union, is used.

\begin{equation}\label{uniquenessProp}
range(\mathbb{P})= \tau_{syntatic} \uplus \tau_{semantic}
\end{equation}

The syntactic uniqueness as represented in Equation \eqref{synType}, is also a disjoint union of the measured attribute type, corresponding to one of integer, float, boolean, or string, and it valid values. The elements of the attribute type set have been determined based on the common primitive types available to the applications at an abstract level. While database schema used by EMR systems, correspond to the SQL datatypes\cite{SQLDataTypes}, the application of semantic reconciliation-on-read strategy, instead necessitates the usage of common programming primitive types. On the other hand, ``validValues" are determined based on what is contained in some sampled portion of the database. It is represented by the set of all possible categorical values (such as values of the attribute with type string which can be used to represent names, address, and others), the minimum and maximum values enclosed within open intervals (``( )") or closed intervals (``[ ]") to represent a range of numerical values ( such as values of attributes with type float or integer ), and boolean values (such as values represented by 1/0 or True/False).
\begin{equation}\label{synType}
\tau_{syntatic}= \left \{ t \mid t \epsilon \left \{ \left \{ integer, float, boolean, string\right \} \uplus validValues \right \} \right \}
\end{equation}

The semantic type is represented by the Equation \eqref{semType}, which is the set of all concepts corresponding to the leaf nodes of the suffix tree, generated from the name of the attribute. Suffix trees are used to divide a string into components, which is a very useful base for quickly identifying the longest common subsequence between a pair of strings and data compression. In our case, these trees provide a manageable list of words, contained with the name of attribute. For each substring, represented by a path of the tree, we then identify the corresponding concept, if it exists, thereby producing a set of concepts which may represent the attribute.

\begin{equation}\label{semType}
\tau_{semantic}= \left \{s \mid s \epsilon \left \{ concept_{i,j} \mid i \leq width(suffixTree(s)) \wedge j \epsilon \left \{ substring_i \right \} \right \} \right \}
\end{equation}

Finally, the set $AA$ of Amplified Attribute (AA), holds all unique attributes of participating schema, as shown in Equation \eqref{uniqueA}. For each element of this set, the attribute is unique if its properties from Equation \eqref{uniquenessProp}, are not similar to any other element of this set. The similarity function ``$\sim$" is a threshold based loose bound, on the disjoint elements of ``$\mathbb{P}$".


\begin{equation}\label{uniqueA}
\exists a \epsilon AA \mid ( \mathbb{P}(a) \wedge \forall b \epsilon AA \mid \mathbb{P}(b) \rightarrow a \sim b)
\end{equation}


As a possible realization of this theoretical set notation form, and from a practical point of view, our methodology is presented in the following subsections.

\subsection*{Schema acquisition}
\label{schema_acq}
In the first step of our semantic reconciliation methodology, we collect medical schema from five distinct EMR storage systems ($S$). These include patient reports from OpenEMR ($s_1$), 100,000 patient records from EMRBOTS ($s_2$) \cite{kartoun2016methodology}, custom database design by Pan et. al($s_3$) for supporting regional clinics and health care centers in China \cite{pan2016design}, clinical knowledge discovery tool MedTAKMI-CDI ($s_4$) \cite{inokuchi2007medtakmi}, and our custom implementation ($s_5$). These five schema follow relational database design, with logical entities, such as demographics, diagnosis, medicine or others, placed into tables which can be further linked to one or more tables. Each of these schema, fulfills the need of their respective information processing applications, however the lack of interoperability is very much visible here. Here, heterogeneity is caused by various factors, including the lack of standard terminologies and different normalization level.
While SNOMED-CT provides a mechanism for identifying the standard codes for clinical terms and LOINC can be used for laboratory related terms, most attribute names are created based on the gut feeling of the database designer. While this behavior is not wrong, it does create syntactic differences in the participating schema. Consider the terms ``name" and ``patientName", which refer to the same attribute of the patient entity, however there is no standard way to represent it, with both being correct. As a result, $s_1$ and $s_3$ use the former representation, while $s_2$ and $s_5$ use the latter. 
Differences in normalization cause semantic differences, due to which some data could be available in one schema but absent in others, such as OpenEMR demographics identifying the patient's residential location using specific attributes like ``Address", ``City", ``State", ``Postal Code", ``Country", and others. Similarly, ``EncounterDate" from $s_5$ is semantically similar to ``BeginDate" of ``openemr\_MedicalProblems" table in $s_1$, ``AdmissionStartDate" of ``LabsCorePopulatedTable" in $s_2$, 
``time" in ``Diagnosis" table of $s_3$, and  ``dateOfAdmission" in ``Diagnosis" and ``CareHistory" tables of $s_4$.
Finally, $s_1$ and $s_3$ have separate tables containing the medicinal prescription details, however the same details are unavailable in $s_2$, $s_4$, $s_5$. Once again, this is not an incorrect behavior since this information, might not be a part of the context or the requirements for these EMR storage systems.
In fact, the change in context of the EMR storage system from the initial time of development to a later stage of collaborative processing systems, is the main cause of heterogeneity.
In order to provide an interoperable solution, it is therefore necessary to provide syntactic and semantic mappings, while taking the data instances (or tuples) into account. Data instances are used to enrich the semantic context of an attribute ($a_i$) by establishing its probable data type and possible value set. 
In our earlier work \cite{Satti2020} $s_1$, $s_2$, and $s_5$ were used to generate over 115 million patient records, which are converted into a semi-structured form and stored in Hadoop Distributed File System (HDFS). We extended the same setup to \textbf{further include}

\subsection*{Schema processing}
\label{schema_prepro}
\textbf{metadata}
In order to process the EMR schema set $S$, we use the methodology shown in \textbf{figAlgorithm}. For each data representation $s_i$, we first serialize its tables into Comma Separated Value (CSV) formatted flat files. Each table has a separate file, which contains the attribute names in the header and the data instances (following the same format and CSV struture), in subsequent lines. Each file is loaded in memory and converted into an algorithmic iterable structure. For each attribute, we then generate the suffix tree, which provides all possible substring representations contained within the attribute name. Then for each node of this suffix tree, if its length is greater than 2 (to avoid lookups for alphabets), we then query UMLS with approximate search strategy.Using Lexical Variant Generation (LVG) this strategy retrieves an expanded list of associated semantic concepts.
If atleast one semantic concept is found for the substring, we replace the substring with the most relevant concept, based on the highest value of the confidence measure, creating the Suffix Concept Tree (SCT). The SCT is structurally smaller than or equal to the suffix tree, since all suffix which are smaller than 2 and those without a qualifying semantic concept, are discarded. Finally for each node of the SCT, the substring from the suffix tree and its corresponding concept is placed into a sentence, which is converted into an embedding vector using \textbf{BioBERT}. This embedding vector represents the domain qualified semantics of the source attribute.

We continue this process, until all substrings have been processed. This is followed by syntactic type check of the attribute, which is based on its data. Here we determine, if the attribute values correspond to one of Integer, Float, Boolean, or String types. Additionally, we collect the values of each corresponding data cell, and determine the set of possible values in each case. 
This semantic enrichment from UMLS concepts and syntactic enrichment from attribute types, is then saved into persistent storage for further processing. 

Meanwhile the process continues for the next attribute, then the next table, and finally the next system, till no further processing is possible. The flat enriched schema, following the design shown in \textbf{figEnrichedSchema} provides the input to our knowledge base. The logically amplified structure of the attribute, AA, is represented in \textbf{figAttributeRepresentation}. Here, the elements of AA are categorized into three parts. ``AttributeContext" contains the metadata conforming to an instance of the attribute's existence,  with its name, container table name and schema name, acting as a pointer, and schema version, source, and recorded date providing the version control features. In this manner, the attribute's fully qualified reference, along with its existential context is identified. ``AttributeType" then encapsulates the data type features of this attribute's instances, with its primitive data type, and values providing a representation of the attribute's instance. Finally, ``Attribute Semantics" holds, the semantic information of this attribute, in the form of its suffix tree, SCT and Tree Embedding.
A pair of AA's with distinct references, are then used as an input to the schema map generation process, which is explained in the next step.

\subsection*{Schema Map generation}
\label{schema_map_gen}
Schema Maps, provide an interoperable bridge between two medical systems ($s_i \wedge s_j$), by identifying the links between their participating attributes. This identification is based on the schema matching process, shown in Algorithm \ref{knowledgeEvolAlgo}, which operates on a pair of amplified attributes, $\left ( aa_i,aa_j \right ) \mid aa_i \epsilon s_i \wedge aa_j \epsilon s_j$, and calculates the similarity score $S$.


\begin{algorithm}
	\textbf{Input}: AmplifiedAttributes $ aa_i, aa_j$ \\
	\textbf{Output}: Similarity $S$ 
	\begin{algorithmic}[1]
		\State Similarity $S = 0$;
		\If{$aa_i.AttributeContext == aa_j.AttributeContext$}:
		\State $S = 1$;
		\Else
		% Checking Attribute Type
		\State Syntactic Similarity $SynSim = 0 $
		\State $at_i = aa_i.AttributeType$
		\State $at_j = aa_j.AttributeType$
		
		\If{$(at_i.DataType == at_j.DataType) || (at_i.DataType \Leftrightarrow at_j.DataType)$}: 
		\State	$SynSim = 1$
		\EndIf
		\State Semantic Similarity $SemSim = 0 $
		\State $\vec{ae_i} =  aa_i.Attribute Semantics
		.TreeEmbedding$
		\State $\vec{ae_j} =  aa_j.Attribute Semantics
		.TreeEmbedding$
		
		\State $SemSim = \frac{\vec{ae_i} \cdot \vec{ae_j}}{\|\vec{ae_i}\|\|\vec{ae_j}\|}$
		
		\State $S = (0.5 * SynSim) + (0.5 * SemSim) $
		\EndIf
		
		\State return $S$
		\caption{Attributes similarity identifier}
		\label{knowledgeEvolAlgo}
	\end{algorithmic}
\end{algorithm}

This algorithmic process starts by comparing the metadata context of the two amplified attributes. The schema name, table name, attribute name, and version are used to establish the context of each attribute, which are then evaluated based on naive string matching of corresponding elements. If the pair refer to the same instance of the amplified attribute the process simply returns 1 as the similarity score. If however, the amplified attribute refer to separate instances, we then apply syntactic and semantic similarity on the pair. Firstly we compare the datatypes of the pair, to determine if they either have the same datatype or the datatypes are convertible. In our current approach \textit{float} and \textit{integer} datatypes are considered convertible, however this step is very much implementation specific and can be extended when the set of types is enhanced with newer data types, such as \textit{short} is convertible to \textit{integer} and vice versa, \textit{bit} is convertible to \textit{boolean} and vice versa, and so on. This test is used to set the ``SynSim" score to ``1", if the datatypes are equal or convertible, and ``0" otherwise. Secondly, we compare the semantic concepts of the two amplified attributes, by applying \textbf{cosine} similarity between their embedding vectors. Since the embedding vector is based on the amalgamation of the suffix strings and their corresponding suffix concepts, \textbf{cosine similarity can give a good measure of the direction of these vectors}. ``SemSim", thereby obtained is then used in conjuction with previously obtained ``SynSim" to calculate the similarity of the two amplified attributes. Using equal weights for syntactic and semantic similarity, we then re-scale their individual values to finally provide a similarity score between ``0" and ``1". 

The schema matching approach represented above, can be further improved by enwrapping the individual calls with memoization, which can avoid redundant checks. However, in practical terms, the creation of the schema map between two amplified attributes is rarer than its application to transform or link two distinct schemas. Additionally, just like its other schema/ontology matching counterparts, the presented Algorithm \ref{knowledgeEvolAlgo}, applies a one-way compression on the various matched factors to produce a similarity measure/ indicator. This operation is very useful for building unsupervised and semi-supervised automated systems, which base their decisions on a measured confidence score. However, in the healthcare domain, knowledge acquisition and application process, requires the use of supervised automated systems, which can provide strong traceability for the decision making process. Summarily, it is not enough for our schema matching process to simply return $S$.

This is where, Ripple Down Rules (RDR) knowledge base, steps in. RDR, by design, provides a safe and traceable data structure for creating, maintaining, and evolving knowledge, in the form of iterpretable rules \cite{compton1992ripple, richards2009two, kim2018rdr}. As \textbf{figRDR} shows, the RDR implementation, which serves as our schema map knowledge base, is very closely related to our schema matching algorithm. Beyond the root node of the RDR, which represents the default condition, at lower depths, syntactic matching characteristics are represented, while semantic matching characteristics, such as the concepts associated with semantic types, follow thereafter. ``AttributeType" elements representing the data types of the attributes as ``string", ``integer", ``double", and ``boolean" are placed in the first case, followed by convertible types, such as the case where the attribute values, in the form of integers are placed in a string (``\textit{1}",``\textit{2}", and so on). The ``Except" clauses then point towards various ``SemanticType", collected from UMLS. Here, the conditional part of the node refers to types such as (``\textit{Sign or Symptom}", ``\textit{Organism Function}", ``\textit{organism Attribute}", and so on), while the conclusion part refers to concepts, such as (``\textit{Fever}", ``\textit{blood pressure}", ``\textit{age}", and so on). In this way, the RDR based knowledge base, provides us with a concrete, memoized, tree data structure, that can be used for schema matching, on the fly. In moving from root to the leaf nodes of this tree, we gain additional matching characteristics, which are all accumulated in the returned results to provide traceability for the decision makers. Additionally, the RDR creation and subsequently evolution process, is dependent on approval by the expert, in determining which cases can be added to the knowledge base. This combination of automated process for building the schema maps, followed by expert intervention to approve the results, enables a safe and consistent knowledge base. 

\subsection*{Schema Map application}
\label{schema_map_evol}

