
\section*{Methodology}
\label{methodology}
Healthcare interoperability, with a focus on non-standard compliant medical schema, is dependent on the generation and validation of schema maps, as discussed above. To this end, the creation of a cohesive workflow is of utmost importance. In our earlier work \cite{Satti2020} we used maximum sequence identification and matching using suffix trees for syntactic matching of two distinctly sources data schemas. This was followed by semantic concept enrichment and subsequently concept matching, for creating rules in the form of schema maps. The simplified mapping functions, thereby created, provided a simple methodology for converting semi-structured medical data, into a non-persisted, interpretable, model form.
In our current methodology we have utilized state-of-the-art machine learning techniques for converting medical schemas into semi-structured form, which is then used to create embedding vectors. These vectors are then used for modeling and creating interpretable rules for applying semantic reconciliation in the form of schema matching and/or transformation. These rules are managed by the RDR which allows fast inference using the relevant knowledge sub-tree, and can well manage knowledge evolution to scale out (by incorporating new schema) and up (by incorporating changes and additions to existing schema)
In essence, the aim here is to define a uniqueness property ($\mathbb{P}$), as shown in Equation \eqref{uniquenessProp}, whereby the semantic and syntactic uniqueness of each attribute as a disjoint union, is used.

\begin{equation}\label{uniquenessProp}
range(\mathbb{P})= \tau_{syntatic} \uplus \tau_{semantic}
\end{equation}

The syntactic uniqueness as represented in Equation \eqref{synType}, is also a disjoint union of the measured attribute type, corresponding to one of integer, float, boolean, or string, and it valid values. The elements of the attribute type set have been determined based on the common primitive types available to the applications at an abstract level. While database schema used by EMR systems, correspond to the SQL datatypes\cite{SQLDataTypes}, the application of semantic reconciliation-on-read strategy, instead necessitates the usage of common programming primitive types. On the other hand, ``validValues" are determined based on what is contained in some sampled portion of the database. It is represented by the set of all possible categorical values (such as values of the attribute with type string which can be used to represent names, address, and others), the minimum and maximum values enclosed within open intervals (``( )") or closed intervals (``[ ]") to represent a range of numerical values ( such as values of attributes with type float or integer ), and boolean values (such as values represented by 1/0 or True/False).
\begin{equation}\label{synType}
\tau_{syntatic}= \left \{ t \mid t \epsilon \left \{ \left \{ integer, float, boolean, string\right \} \uplus validValues \right \} \right \}
\end{equation}

The semantic type is represented by the Equation \eqref{semType}, which is the set of all concepts corresponding to the leaf nodes of the suffix tree, generated from the name of the attribute. Suffix trees are used to divide a string into components, which is a very useful base for quickly identifying the longest common subsequence between a pair of strings and data compression. In our case, these trees provide a manageable list of words, contained with the name of attribute. For each substring, represented by a path of the tree, we then identify the corresponding concept, if it exists, thereby producing a set of concepts which may represent the attribute.

\begin{equation}\label{semType}
\tau_{semantic}= \left \{s \mid s \epsilon \left \{ concept_{i,j} \mid i \leq width(suffixTree(s)) \wedge j \epsilon \left \{ substring_i \right \} \right \} \right \}
\end{equation}

Finally, the set $AA$ of Amplified Attribute (AA), holds all unique attributes of participating schema, as shown in Equation \eqref{uniqueA}. For each element of this set, the attribute is unique if its properties from Equation \eqref{uniquenessProp}, are not similar to any other element of this set. The similarity function ``$\sim$" is a threshold based loose bound, on the disjoint elements of ``$\mathbb{P}$".


\begin{equation}\label{uniqueA}
\exists a \epsilon AA \mid ( \mathbb{P}(a) \wedge \forall b \epsilon AA \mid \mathbb{P}(b) \rightarrow a \sim b)
\end{equation}


As a possible realization of this theoretical set notation form, and from a practical point of view, our methodology is presented in the following subsections.

\subsection*{Schema acquisition}
\label{schema_acq}
In the first step of our semantic reconciliation methodology, we collect medical schema from five distinct EMR storage systems ($S$). These include patient reports from OpenEMR ($s_1$), 100,000 patient records from EMRBOTS ($s_2$) \cite{kartoun2016methodology}, custom database design by Pan et. al($s_3$) for supporting regional clinics and health care centers in China \cite{pan2016design}, clinical knowledge discovery tool MedTAKMI-CDI ($s_4$) \cite{inokuchi2007medtakmi}, and our custom implementation ($s_5$). These five schema follow relational database design, with logical entities, such as demographics, diagnosis, medicine or others, placed into tables which can be further linked to one or more tables. Each of these schema, fulfills the need of their respective information processing applications, however the lack of interoperability is very much visible here. Here, heterogeneity is caused by various factors, including the lack of standard terminologies and different normalization level.
While SNOMED-CT provides a mechanism for identifying the standard codes for clinical terms and LOINC can be used for laboratory related terms, most attribute names are created based on the gut feeling of the database designer. While this behavior is not wrong, it does create syntactic differences in the participating schema. Consider the terms ``name" and ``patientName", which refer to the same attribute of the patient entity, however there is no standard way to represent it, with both being correct. As a result, $s_1$ and $s_3$ use the former representation, while $s_2$ and $s_5$ use the latter. 
Differences in normalization cause semantic differences, due to which some data could be available in one schema but absent in others, such as OpenEMR demographics identifying the patient's residential location using specific attributes like ``Address", ``City", ``State", ``Postal Code", ``Country", and others. Similarly, ``EncounterDate" from $s_5$ is semantically similar to ``BeginDate" of ``openemr\_MedicalProblems" table in $s_1$, ``AdmissionStartDate" of ``LabsCorePopulatedTable" in $s_2$, 
``time" in ``Diagnosis" table of $s_3$, and  ``dateOfAdmission" in ``Diagnosis" and ``CareHistory" tables of $s_4$.
Finally, $s_1$ and $s_3$ have separate tables containing the medicinal prescription details, however the same details are unavailable in $s_2$, $s_4$, $s_5$. Once again, this is not an incorrect behavior since this information, might not be a part of the context or the requirements for these EMR storage systems.
In fact, the change in context of the EMR storage system from the initial time of development to a later stage of collaborative processing systems, is the main cause of heterogeneity.
In order to provide an interoperable solution, it is therefore necessary to provide syntactic and semantic mappings, while taking the data instances (or tuples) into account. Data instances are used to enrich the semantic context of an attribute ($a_i$) by establishing its probable data type and possible value set. 
In our earlier work \cite{Satti2020} $s_1$, $s_2$, and $s_5$ were used to generate over 115 million patient records, which are converted into a semi-structured form and stored in Hadoop Distributed File System (HDFS). We extended the same setup to \textbf{further include}

\subsection*{Schema processing}
\label{schema_prepro}

In order to process the EMR schema set $S$, we use the methodology shown in \textbf{figAlgorithm}. For each data representation $s_i$, we first serialize its tables into Comma Separated Value (CSV) formatted flat files. Each table has a separate file, which contains the attribute names in the header and the data instances (following the same format and CSV struture), in subsequent lines. Each file is loaded in memory and converted into an algorithmic iterable structure. For each attribute, we then generate the suffix tree, which provides all possible substring representations contained within the attribute name. Then for each node of this suffix tree, if its length is greater than 2 (to avoid lookups for alphabets), we then query UMLS with approximate search strategy.Using Lexical Variant Generation (LVG) this strategy retrieves an expanded list of associated semantic concepts.
If atleast one semantic concept is found for the substring, we replace the substring with the most relevant concept, based on the highest value of the confidence measure, creating the Suffix Concept Tree (SCT). The SCT is structurally smaller than or equal to the suffix tree, since all suffix which are smaller than 2 and those without a qualifying semantic concept, are discarded. Finally for each node of the SCT, the substring from the suffix tree and its corresponding concept is placed into a sentence, which is converted into an embedding vector using \textbf{BioBERT}. This embedding vector represents the domain qualified semantics of the source attribute.

We continue this process, until all substrings have been processed. This is followed by syntactic type check of the attribute, which is based on its data. Here we determine, if the attribute values correspond to one of Integer, Float, Boolean, or String types. Additionally, we collect the values of each corresponding data cell, and determine the set of possible values in each case. 
This semantic enrichment from UMLS concepts and syntactic enrichment from attribute types, is then saved into persistent storage for further processing. 

Meanwhile the process continues for the next attribute, then the next table, and finally the next system, till no further processing is possible. The flat enriched schema, following the design shown in \textbf{figEnrichedSchema} provides the input to our knowledge base. The logically amplified structure of the attribute, AA, is represented in \textbf{figAttributeRepresentation}. Here, the elements of AA are categorized into three parts. ``AttributeContext" contains the metadata conforming to an instance of the attribute's existence,  with its name, container table name and schema name, acting as a pointer, and schema version, source, and recorded date providing the version control features. In this manner, the attribute's fully qualified reference, along with its existential context is identified. ``AttributeType" then encapsulates the data type features of this attribute's instances, with its primitive data type, and values providing a representation of the attribute's instance. Finally, ``Attribute Semantics" holds, the semantic information of this attribute, in the form of its suffix tree, SCT and Tree Embedding.
A pair of AA's with distinct references, are then used as an input to the schema map generation process, which is explained in the next step.

\subsection*{Schema Map generation}
\label{schema_map_gen}
%Schema Maps, provide an interoperable bridge between two medical systems, by identifying the links between their participating attributes. This identification is based on the schema matching process, which process the cartesian product  of the two systems (le$).



attribute set of bothTheoretically, for ``n" attributes in $s_i$ and ``m" attributes in $s_j$

\subsection*{Schema Map evolution}
\label{schema_map_evol}

